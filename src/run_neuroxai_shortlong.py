#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
run_neuroxai_eegnet.py
======================
Pipeline: 60-ch Baseline â†’ (å¯é€‰) NeuroXAI é€‰é€šé“ â†’ Top-K è¯„ä¼°

æœ¬ç‰ˆæœ¬ä¿®æ­£ & å¢å¼ºï¼š
1. **A1**  ä½¿ç”¨ GroupKFold(10) æŒ‰ trial-ID åˆ†æ®µï¼Œé¿å…åŒä¸€ trial çš„çª—å£åŒæ—¶è½å…¥
   è®­ç»ƒ / éªŒè¯é›†ï¼Œbaseline ä¸å†è™šé«˜ã€‚
2. **A3**  æ¯ä¸ª subject é¢å¤–è¯„ä¼°
      â€¢ FBCSP æ’åå‰ 30 é€šé“ (acc30)
      â€¢ NeuroXAI æœ€ç»ˆ Top-16 é€šé“ (acc16)
   ä¸ 60-ch baseline å¹¶åˆ—æ‰“å°ï¼Œä¾¿äºåˆ¤æ–­é—®é¢˜å‡ºåœ¨é¢„ç­›è¿˜æ˜¯è§£é‡Šå™¨ã€‚
"""
print('hello w')
import argparse, json, random, time, warnings
from pathlib import Path

import numpy as np
import torch, torch.nn as nn
from mne.decoding import CSP
from neuroxai.explanation import BrainExplainer, GlobalBrainExplainer
from scipy.io import loadmat
from scipy.signal import butter, filtfilt, iirnotch
from sklearn.decomposition import FastICA
from sklearn.model_selection import GroupKFold
from tqdm import tqdm

# ---------------- å…¨å±€è¶…å‚ ----------------
SEED = 0
FS = 256
WIN_S, STEP_S = 2.0, .5                 # 2-s çª—ï¼Œ0.5-s æ­¥
WIN, STEP = int(WIN_S * FS), int(STEP_S * FS)
EPOCH_BASE = 100                        # baseline è®­ç»ƒ epoch
EPOCH_CV   = 60                         # é€šé“é€‰æ‹©å epoch
BATCH      = 128
THR_BASE   = .60                        # baseline ä½äºæ­¤é˜ˆå€¼ â†’ è·³é€šé“é€‰æ‹©
FBCSP_BANDS = [(4 + i * 4, 8 + i * 4) for i in range(8)]   # 4-40 Hz
CANDIDATE   = 30                        # FBCSP å€™é€‰æ•°
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# reproducibility
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

ROOT = Path(__file__).resolve().parent.parent
DATA = ROOT / "data/Short_Long_words"
FILES = sorted(f for f in DATA.glob("*.mat") if "_8s" not in f.name)

# ---------- é€šé“ä¿¡æ¯ ----------
first = loadmat(FILES[0], simplify_cells=True)
k0 = next(k for k in first if k.endswith("last_beep"))
n_tot = first[k0][0][0].shape[0]          # 64

DROP_FIXED = {0, 9, 32, 63}
keep_idx = [i for i in range(n_tot) if i not in DROP_FIXED]

orig_names = [f"Ch{i}" for i in range(n_tot)]
if "ch_names" in first:
    orig_names = [str(s).strip() for s in first["ch_names"]][:n_tot]

CHAN_NAMES = [orig_names[i] for i in keep_idx]
N_CH = len(CHAN_NAMES)
print(f"å¯ç”¨é€šé“ = {N_CH} | device = {DEVICE}")

# ---------- æ»¤æ³¢ ----------
bp_b, bp_a = butter(4, [4, 40], fs=FS, btype="band")
nt_b, nt_a = iirnotch(60, 30, fs=FS)

def preprocess(sig):
    sig = sig[keep_idx]                              # (C,T)
    sig = filtfilt(nt_b, nt_a, sig, axis=1)
    sig = filtfilt(bp_b, bp_a, sig, axis=1)
    sig -= sig.mean(1, keepdims=True)
    sig /= sig.std(1, keepdims=True) + 1e-6
    return sig.astype(np.float32)

def slide(sig, trial_id):
    """è¿”å›è¯¥ trial åˆ’çª—åçš„ window åˆ—è¡¨ åŠ å¯¹åº” trial-ID åˆ—è¡¨"""
    wins, gids = [], []
    for st in range(0, sig.shape[1] - WIN + 1, STEP):
        wins.append(sig[:, st:st + WIN])
        gids.append(trial_id)
    return wins, gids

# ---------- FastICA ----------
def fast_ica_all(trials, n_comp=None):
    C, T = trials[0].shape
    X = np.concatenate(trials, 1).T        # (samples, C)
    n_comp = C if n_comp is None else n_comp
    ica = FastICA(n_components=n_comp, whiten='unit-variance',
                  random_state=SEED, max_iter=300)
    _ = ica.fit_transform(X)
    return np.asarray([ica.transform(s.T).T.astype(np.float32) for s in trials])

# ---------- FBCSP æ’åº ----------
def fbcsp_rank(trials, labels):
    score = np.zeros(N_CH)
    for low, high in FBCSP_BANDS:
        b, a = butter(4, [low, high], fs=FS, btype='band')
        fb = filtfilt(b, a, trials, axis=2)
        csp = CSP(n_components=2, reg='ledoit_wolf', log=False)
        csp.fit(fb.astype(np.float64), labels)
        w = csp.filters_                       # (2,C)
        score += np.abs(w[0]) + np.abs(w[-1])
    return np.argsort(score)[::-1]

# ---------- EEGNet ----------
class EEGNet(nn.Module):
    def __init__(self, C: int, n_cls: int = 2, dropout=0.25):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(1, 8, (1, 64), padding=(0, 32), bias=False), nn.BatchNorm2d(8), nn.ReLU(),
            nn.Conv2d(8, 16, (C, 1), groups=8, bias=False),        nn.BatchNorm2d(16),
            nn.ReLU(), nn.AvgPool2d((1, 4)), nn.Dropout(dropout),
            nn.Conv2d(16, 16, (1, 16), padding=(0, 8), bias=False), nn.BatchNorm2d(16),
            nn.ReLU(), nn.AvgPool2d((1, 8)), nn.Dropout(dropout),
            nn.Conv2d(16, 16, 1, bias=False), nn.ReLU()
        )
        self.gap  = nn.AdaptiveAvgPool2d((1, 1))
        self.head = nn.Linear(16, n_cls)

    def forward(self, x):
        x = self.block(x)          # (B,16,?,?)
        x = self.gap(x).flatten(1)
        return self.head(x)

def train_net(X, y, epochs, lr=1e-3, dropout=0.25):
    net = EEGNet(X.shape[2], dropout=dropout).to(DEVICE)
    opt = torch.optim.AdamW(net.parameters(), lr, weight_decay=1e-4)
    sch = torch.optim.lr_scheduler.StepLR(opt, 50, 0.5)
    cri = nn.CrossEntropyLoss()
    net.train()
    for _ in range(epochs):
        idx = torch.randperm(len(X), device=DEVICE)
        for beg in range(0, len(idx), BATCH):
            sl = idx[beg:beg + BATCH]
            opt.zero_grad()
            loss = cri(net(X[sl]), y[sl]); loss.backward(); opt.step()
        sch.step()
    return net

def eval_net(net, X, y_np, g_np):
    net.eval(); preds = []
    with torch.no_grad():
        for beg in range(0, len(X), BATCH):
            preds.append(net(X[beg:beg + BATCH]).argmax(1).cpu())
    preds = np.concatenate(preds)
    # trial-level majority vote
    vote = {}
    for p, i in zip(preds, g_np):
        vote.setdefault(i, []).append(p)
    trial_pred = {i: max(set(v), key=v.count) for i, v in vote.items()}
    return np.mean([trial_pred[i] == int(y_np[g_np == i][0]) for i in trial_pred])

# ---------- NeuroXAI importance ----------
# ---------- NeuroXAI importance ----------
def neuroxai_imp(base, trials, labels, cand_idx, n_samples):
    """
    è°ƒç”¨ BrainExplainer + GlobalBrainExplainer è®¡ç®—å…¨å±€é€šé“é‡è¦åº¦
    è¿”å›é•¿åº¦ = 60 çš„ importance å‘é‡ï¼ˆæ²¡è¢«é€‰è¿› cand_idx çš„ä½ç½®ä¸º 0ï¼‰
    """
    def clf(batch):
        # batch: (n, 1, C, T) â†’ softmax æ¦‚ç‡ (n, n_cls)
        t = torch.tensor(batch[:, None, :, :], dtype=torch.float32, device=DEVICE)
        with torch.no_grad():
            out = base(t)
        return torch.softmax(out, 1).cpu().numpy()

    brain = BrainExplainer(25, ['short', 'long'])
    gexp  = GlobalBrainExplainer(brain)

    # å…³é”®ï¼šå‚æ•°åå¿…é¡»æ˜¯ num_samples ï¼ğŸ‘‡
    gexp.explain_instance(
        x=trials[:, :, cand_idx],      # åªé€ 30 ä¸ªå€™é€‰é€šé“
        y=labels,
        classifier_fn=clf,
        num_samples=n_samples,         # â† ä¿®æ­£è¿™é‡Œ
        replacement_method='mean'
    )

    imp = np.zeros(N_CH)
    imp[cand_idx] = [
        gexp.explain_global_channel_importance().get(i, 0.0)
        for i in range(len(cand_idx))
    ]
    return imp

# ---------- ä¸»æµç¨‹ ----------
def main(k_list, n_samples, use_ica=True):
    all_res = {}
    gkf = GroupKFold(10)

    for subj_i, matf in enumerate(FILES, 1):
        print(f"\n=== Subject {subj_i}/{len(FILES)} ({matf.name}) ===")
        t0 = time.time()

        # ---------- è¯»å– & é¢„å¤„ç† ----------
        m   = loadmat(matf, simplify_cells=True)
        key = next(k for k in m if k.endswith("last_beep"))
        trials = [preprocess(tr) for cls in m[key] for tr in cls]
        labels = [cls for cls, tset in enumerate(m[key]) for _ in tset]
        trials = np.asarray(trials)
        labels = np.asarray(labels, dtype=int)
        if use_ica:
            trials = fast_ica_all(trials)
        print(f"Loaded  trials={len(trials)} | {time.time()-t0:.1f}s")

        # ---------- slide æ‰€æœ‰ trial ----------
        Xw, Yn, Gn = [], [], []
        for tid, (sig, lab) in enumerate(zip(trials, labels)):
            wins, gids = slide(sig, tid)
            Xw.extend(wins)
            Yn.extend([lab] * len(wins))
            Gn.extend(gids)
        Xw = np.stack(Xw)                         # (n_win, C, T)
        Yn = np.asarray(Yn, dtype=int)
        Gn = np.asarray(Gn, dtype=int)
        X_t = torch.tensor(Xw[:, None, :, :], device=DEVICE)
        Y_t = torch.tensor(Yn, device=DEVICE)

        # ---------- Baseline 60-ch 10-fold ----------
        print("Baseline 60-ch â€¦")
        cv_acc = []
        for tr, te in gkf.split(Xw, Yn, groups=Gn):
            net = train_net(X_t[tr], Y_t[tr], EPOCH_BASE)
            cv_acc.append(eval_net(net, X_t[te], Yn[te], Gn[te]))
        acc0, std0 = float(np.mean(cv_acc)), float(np.std(cv_acc))
        print(f"Baseline = {acc0:.3f} Â± {std0:.3f}")
        sub_res = {"baseline": [acc0, std0]}

        if acc0 < THR_BASE:
            print("â†³ baseline < threshold, skip channel selection")
            all_res[f"sub{subj_i:02d}"] = sub_res
            continue

        # ---------- FBCSP é¢„ç­› ----------
        cand_idx = fbcsp_rank(trials, labels)[:CANDIDATE]
        print("FBCSP-30:", [CHAN_NAMES[i] for i in cand_idx])

        # ---------- è¯„ä¼° FBCSP Top-30 ----------
        def cv_acc_subset(sel_idx, epochs):
            Xi = X_t[:, :, sel_idx, :]
            scores = []
            for tr, te in gkf.split(Xi, Yn, groups=Gn):
                net_tmp = train_net(Xi[tr], Y_t[tr], epochs)
                scores.append(eval_net(net_tmp, Xi[te], Yn[te], Gn[te]))
            return float(np.mean(scores)), float(np.std(scores))

        acc30, std30 = cv_acc_subset(cand_idx, EPOCH_BASE)
        print(f"FBCSP-30 = {acc30:.3f} Â± {std30:.3f}")
        sub_res["fbcsp30"] = [acc30, std30]

        # ---------- NeuroXAI ----------
        print("NeuroXAI â€¦")
        # å…ˆç”¨ baseline net ä½œä¸ºä»£ç†è®¡ç®—é‡è¦åº¦
        imp   = neuroxai_imp(net, trials, labels, cand_idx, n_samples)
        order = np.argsort(-imp)

        # Top-16 å…¨å±€éªŒè¯
        acc16, std16 = cv_acc_subset(order[:16], EPOCH_CV)
        print(f"Top-16   = {acc16:.3f} Â± {std16:.3f}")
        sub_res["top16"] = [acc16, std16]

        # ---------- Top-K (4/8/16/32) ----------
        for K in k_list:
            sel = order[:K]
            accK, stdK = cv_acc_subset(sel, EPOCH_CV)
            sub_res[str(K)] = [accK, stdK]
            print(f"Top-{K:<2}  = {accK:.3f} Â± {stdK:.3f}")

        all_res[f"sub{subj_i:02d}"] = sub_res

    # ---------- ä¿å­˜ ----------
    out = ROOT / "results/subject_dep_neuroxai_eegnet.json"
    json.dump(all_res, open(out, "w"), indent=2)
    print("\nâœ” ç»“æœå†™å…¥", out)

# ---------- CLI ----------
if __name__ == "__main__":
    warnings.filterwarnings("ignore")
    pa = argparse.ArgumentParser()
    pa.add_argument("--k", type=int, nargs="+", default=[4, 8, 16, 32],
                    help="è¯„ä¼°çš„ Top-K åˆ—è¡¨")
    pa.add_argument("--n_samples", type=int, default=800,
                    help="NeuroXAI éšæœºæ‰°åŠ¨æ ·æœ¬æ•°")
    pa.add_argument("--no_ica", action="store_true",
                    help="ä¸ç»™ trial åš ICA")
    pa.add_argument("--json", default=None, help=argparse.SUPPRESS)

    # åªè§£æå·²çŸ¥å‚æ•°ï¼Œå…¶ä½™ä¸¢å¼ƒï¼›è¿™æ ·å³ä½¿å†™äº† --json ä¹Ÿä¸ä¼šæŠ¥é”™
    args, _ = pa.parse_known_args()
    main(args.k, args.n_samples, use_ica=not args.no_ica)